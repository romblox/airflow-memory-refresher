import os
import zipfile
from datetime import datetime

from airflow.sdk import dag, task
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator
from airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

from constants import UPLOAD_DIR, DOWNLOAD_DIR

def download_from_s3():
    hook = S3Hook(aws_conn_id="aws-free-tier")
    return hook.download_file(
        bucket_name="s3-shkiper-private",
        key="images/YYachts-Y7.jpg",
        filename=str(DOWNLOAD_DIR / "YYachts-Y7.jpg"),
    )


@dag
def list_s3_objects():
    list_objects = S3ListOperator(
        task_id="list_objects",
        bucket="s3-shkiper-private",
        prefix="images/",
        delimiter="/",
        aws_conn_id="aws-free-tier",
    )

    @task
    def print_objects(objects):
        print(">>>>>>>>>>>>>>>>>>>>>>>>>>>>")
        for obj in objects:
            print(obj)
        print(">>>>>>>>>>>>>>>>>>>>>>>>>>>>")
        return objects


    create_local_to_s3_job = LocalFilesystemToS3Operator(
        task_id="create_local_to_s3_job",
        filename=str(UPLOAD_DIR / "YYachts-Y7.jpg"),
        dest_key='images/YYachts-Y7n.jpg',
        dest_bucket="s3-shkiper-private",
        replace=True,
        aws_conn_id="aws-free-tier",
    )

    @task
    def download_file_from_s3_bucker_to_local_computer():
        hook = S3Hook(aws_conn_id="aws-free-tier")



        result = hook.download_file(
            bucket_name="s3-shkiper-private",
            key="images/YYachts-Y7n.jpg",
            local_path=str(DOWNLOAD_DIR),
            preserve_file_name=True,
            use_autogenerated_subdir=False,
        )

        print(">"*44)
        print(result)
        print(">"*44)

        return result



    # https://github.com/apache/airflow/blob/b808dd8d82d5407da31fd6085c403dbb3b0fa3c1/providers/amazon/tests/system/amazon/aws/example_s3.py#L235
    # copy_object = S3CopyObjectOperator(
    #     task_id="copy_object",
    #     source_bucket_name=bucket_name,
    #     dest_bucket_name=bucket_name_2,
    #     source_bucket_key=key,
    #     dest_bucket_key=key_2,
    # )

    # https://github.com/apache/airflow/blob/b808dd8d82d5407da31fd6085c403dbb3b0fa3c1/providers/amazon/tests/system/amazon/aws/example_s3.py#L245
    # file_transform = S3FileTransformOperator(
    #     task_id="file_transform",
    #     source_s3_key=f"s3://{bucket_name}/{key}",
    #     dest_s3_key=f"s3://{bucket_name_2}/{key_2}",
    #     # Use `cp` command as transform script as an example
    #     transform_script="cp",
    #     replace=True,
    # )

    # https://github.com/apache/airflow/blob/b808dd8d82d5407da31fd6085c403dbb3b0fa3c1/providers/amazon/tests/system/amazon/aws/example_s3.py#L265
    # delete_objects = S3DeleteObjectsOperator(
    #     task_id="delete_objects",
    #     bucket=bucket_name_2,
    #     keys=key_2,
    # )

    @task
    def create_zip_archive(file_path):
        # Create a zip file with timestamp to avoid overwriting
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        zip_filename = f"archive_{timestamp}.zip"
        zip_path = str(DOWNLOAD_DIR / zip_filename)

        # Create zip archive
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # Add the file to the zip
            zipf.write(file_path, os.path.basename(file_path))

        print(f"Created zip archive at: {zip_path}")
        return zip_path

    @task
    def upload_zip_to_s3(zip_path):
        # Upload the zip file to S3
        hook = S3Hook(aws_conn_id="aws-free-tier")
        zip_filename = os.path.basename(zip_path)
        s3_key = f"archives/{zip_filename}"

        hook.load_file(
            filename=zip_path,
            key=s3_key,
            bucket_name="s3-shkiper-private",
            replace=True
        )

        print(f"Uploaded zip archive to S3: s3://s3-shkiper-private/{s3_key}")
        return s3_key

    # Set up the task dependencies
    downloaded_file = download_file_from_s3_bucker_to_local_computer()
    zip_file = create_zip_archive(downloaded_file)
    uploaded_zip = upload_zip_to_s3(zip_file)

    # Update your DAG's workflow
    print_objects(list_objects.output) >> create_local_to_s3_job >> downloaded_file >> zip_file >> uploaded_zip

    # print_objects(list_objects.output) >> create_local_to_s3_job >> download_file_from_s3_bucker_to_local_computer()



list_s3_objects()
