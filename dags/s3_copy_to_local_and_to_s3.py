import os
import zipfile
from datetime import datetime

from airflow.sdk import dag, task
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator
from airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

from constants import UPLOAD_DIR, DOWNLOAD_DIR


@dag
def s3_to_local_zip_and_to_s3():
    # list_objects = S3ListOperator(
    #     task_id="s3_to_local_zip_and_to_s3",
    #     bucket="s3-shkiper-private",
    #     prefix="images/",
    #     delimiter="/",
    #     aws_conn_id="aws-free-tier",
    # )
    #
    # @task
    # def print_objects(objects):
    #     print(">>>>>>>>>>>>>>>>>>>>>>>>>>>>")
    #     for obj in objects:
    #         print(obj)
    #     print(">>>>>>>>>>>>>>>>>>>>>>>>>>>>")
    #     return objects

    copy_local_file_to_s3 = LocalFilesystemToS3Operator(
        task_id="copy_local_file_to_s3",
        filename=str(UPLOAD_DIR / "YYachts-Y7.jpg"),
        dest_key='images/YYachts-Y7n.jpg',
        dest_bucket="s3-shkiper-private",
        replace=True,
        aws_conn_id="aws-free-tier",
    )

    @task
    def download_file_from_s3_to_local_host():
        hook = S3Hook(aws_conn_id="aws-free-tier")
        result = hook.download_file(
            bucket_name="s3-shkiper-private",
            key="images/YYachts-Y7n.jpg",
            local_path=str(DOWNLOAD_DIR),
            preserve_file_name=True,
            use_autogenerated_subdir=False,
        )

        return result



    # https://github.com/apache/airflow/blob/b808dd8d82d5407da31fd6085c403dbb3b0fa3c1/providers/amazon/tests/system/amazon/aws/example_s3.py#L235
    # copy_object = S3CopyObjectOperator(
    #     task_id="copy_object",
    #     source_bucket_name=bucket_name,
    #     dest_bucket_name=bucket_name_2,
    #     source_bucket_key=key,
    #     dest_bucket_key=key_2,
    # )

    # https://github.com/apache/airflow/blob/b808dd8d82d5407da31fd6085c403dbb3b0fa3c1/providers/amazon/tests/system/amazon/aws/example_s3.py#L245
    # file_transform = S3FileTransformOperator(
    #     task_id="file_transform",
    #     source_s3_key=f"s3://{bucket_name}/{key}",
    #     dest_s3_key=f"s3://{bucket_name_2}/{key_2}",
    #     # Use `cp` command as transform script as an example
    #     transform_script="cp",
    #     replace=True,
    # )

    # https://github.com/apache/airflow/blob/b808dd8d82d5407da31fd6085c403dbb3b0fa3c1/providers/amazon/tests/system/amazon/aws/example_s3.py#L265
    # delete_objects = S3DeleteObjectsOperator(
    #     task_id="delete_objects",
    #     bucket=bucket_name_2,
    #     keys=key_2,
    # )

    @task
    def generate_timestamp_file():
        # Create a text file with the current time
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        filename = f"timestamp_{datetime.now().strftime('%Y%m%d%H%M%S')}.txt"
        file_path = str(DOWNLOAD_DIR / filename)

        # Write the content to the file
        with open(file_path, 'w') as f:
            f.write(f"Currently is {timestamp} and we create this file.")

        print(f"Created timestamp file at: {file_path}")
        return file_path

    # @task
    # def create_zip_archive(file_path):
    #     # Create a zip file with timestamp to avoid overwriting
    #     timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    #     zip_filename = f"archive_{timestamp}.zip"
    #     zip_path = str(DOWNLOAD_DIR / zip_filename)
    #
    #     # Create zip archive
    #     with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
    #         # Add the file to the zip
    #         zipf.write(file_path, os.path.basename(file_path))
    #
    #     print(f"Created zip archive at: {zip_path}")
    #     return zip_path

    @task
    def create_zip_archive(file_path, timestamp_file_path):
        # Create a zip file with timestamp to avoid overwriting
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        zip_filename = f"archive_{timestamp}.zip"
        zip_path = str(DOWNLOAD_DIR / zip_filename)

        # Create zip archive
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # Add the downloaded file to the zip
            zipf.write(file_path, os.path.basename(file_path))

            # Add the timestamp text file to the zip
            zipf.write(timestamp_file_path, os.path.basename(timestamp_file_path))

        print(f"Created zip archive at: {zip_path} with 2 files")
        return zip_path

    @task
    def upload_zip_to_s3(zip_path):
        # Upload the zip file to S3
        hook = S3Hook(aws_conn_id="aws-free-tier")
        zip_filename = os.path.basename(zip_path)
        s3_key = f"archives/{zip_filename}"

        hook.load_file(
            filename=zip_path,
            key=s3_key,
            bucket_name="s3-shkiper-private",
            replace=True
        )

        print(f"Uploaded zip archive to S3: s3://s3-shkiper-private/{s3_key}")
        return s3_key

    # Set up the task dependencies
    downloaded_file = download_file_from_s3_to_local_host()
    timestamp_file = generate_timestamp_file()
    zip_file = create_zip_archive(downloaded_file, timestamp_file)
    uploaded_zip = upload_zip_to_s3(zip_file)

    # Update your DAG's workflow
    copy_local_file_to_s3 >> downloaded_file >> timestamp_file >> zip_file >> uploaded_zip


s3_to_local_zip_and_to_s3()
